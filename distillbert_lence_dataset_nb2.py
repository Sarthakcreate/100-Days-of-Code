# -*- coding: utf-8 -*-
"""DistillBERT_LenCE_Dataset_nb2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BlGdha20fYHh-eR1Kcl1XEZFo6D8opPJ
"""

!pip install transformers torch pandas tqdm seqeval torchinfo

import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import DistilBertTokenizerFast, DistilBertForTokenClassification, AdamW
from seqeval.metrics import classification_report
import numpy as np
from tqdm import tqdm
from typing import List, Tuple, Dict
import re

# Constants
MAX_LEN = 128
TRAIN_BATCH_SIZE = 32
VALID_BATCH_SIZE = 16
EPOCHS = 3
LEARNING_RATE = 2e-5
MAX_GRAD_NORM = 1.0

# Define label mappings at the module level
label2id = {'EN': 0, 'HI': 1, 'O': 2}
id2label = {v: k for k, v in label2id.items()}

## Custom DataSet for LenCE conll files
class CONLLDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        word_labels = self.labels[idx]

        encoding = self.tokenizer(
            text.split(),
            is_split_into_words=True,
            return_offsets_mapping=True,
            padding='max_length',
            truncation=True,
            max_length=self.max_len
        )

        word_ids = encoding.word_ids()
        label_ids = [-100 if word_id is None else label2id[word_labels[word_id]]
                     for word_id in word_ids]

        return {
            'input_ids': torch.tensor(encoding['input_ids'], dtype=torch.long),
            'attention_mask': torch.tensor(encoding['attention_mask'], dtype=torch.long),
            'labels': torch.tensor(label_ids, dtype=torch.long)
        }

"""### Load conll file"""

def read_conll_file(file_path: str) -> Tuple[List[str], List[List[str]]]:
    texts = []
    all_labels = []

    current_sentence = []
    current_labels = []

    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()

            if line.startswith('# sent_enum'):
                if current_sentence:
                    texts.append(' '.join(current_sentence))
                    all_labels.append(current_labels)
                    current_sentence = []
                    current_labels = []
                continue

            if not line:
                continue

            parts = line.split('\t')
            if len(parts) >= 2:
                word, label = parts[0], parts[1]
                current_sentence.append(word)

                # Map the CONLL labels to our model's labels
                if label.lower() in ['lang1', 'lang1a']:
                    mapped_label = 'EN'
                elif label.lower() in ['lang2', 'mixed']:
                    mapped_label = 'HI'
                else:
                    mapped_label = 'O'

                current_labels.append(mapped_label)

    # Add the last sentence if exists
    if current_sentence:
        texts.append(' '.join(current_sentence))
        all_labels.append(current_labels)

    return texts, all_labels

"""## Training and evaluation function"""

def train_epoch(model, data_loader, optimizer, device, scheduler):
    model.train()
    total_loss = 0

    progress_bar = tqdm(data_loader, desc='Training')
    for batch in progress_bar:
        optimizer.zero_grad()

        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )

        loss = outputs.loss
        total_loss += loss.item()

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)

        optimizer.step()
        scheduler.step()

        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})

    return total_loss / len(data_loader)

#this is not modified for non NER format data
# def evaluate(model, data_loader, device):
#     model.eval()
#     predictions = []
#     true_labels = []

#     with torch.inference_mode():
#         for batch in data_loader:
#             input_ids = batch['input_ids'].to(device)
#             attention_mask = batch['attention_mask'].to(device)
#             labels = batch['labels']

#             outputs = model(input_ids=input_ids, attention_mask=attention_mask)
#             logits = outputs.logits
#             pred = torch.argmax(logits, dim=2)

#             pred_list = pred.detach().cpu().numpy()
#             labels_list = labels.cpu().numpy()

#             for p, l, mask in zip(pred_list, labels_list, attention_mask):
#                 # Fixed: Using index from enumerate instead of undefined 'i'
#                 pred_labels = [id2label[p_i] for idx, (p_i, m) in enumerate(zip(p, mask))
#                               if m == 1 and l[idx] != -100]
#                 true_labels_subset = [id2label[l_i] for l_i in l if l_i != -100]

#                 predictions.append(pred_labels)
#                 true_labels.append(true_labels_subset)

#     return classification_report(true_labels, predictions)

def evaluate(model, data_loader, device):
    model.eval()
    predictions = []
    true_labels = []

    with torch.no_grad():
        for batch in data_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels']

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            pred = torch.argmax(logits, dim=2)

            pred_list = pred.detach().cpu().numpy()
            labels_list = labels.cpu().numpy()

            for p, l, mask in zip(pred_list, labels_list, attention_mask):
                pred_labels = [convert_to_ner_tag(id2label[p_i]) for idx, (p_i, m) in enumerate(zip(p, mask))
                              if m == 1 and l[idx] != -100]
                true_labels_subset = [convert_to_ner_tag(id2label[l_i]) for l_i in l if l_i != -100]

                predictions.append(pred_labels)
                true_labels.append(true_labels_subset)

    return classification_report(true_labels, predictions)

def convert_to_ner_tag(tag):
    """Convert plain language tags to NER format"""
    if tag in ['EN', 'HI']:
        return f'B-{tag}'  # or could use 'I-{tag}' - the choice depends on your needs
    return 'O'

"""### Predict language Tags for each input text"""

def predict_language_tags(model, tokenizer, text, device):
    """Predict language tags for each word in the input text."""
    words = text.split()
    encoding = tokenizer(
        words,
        is_split_into_words=True,
        return_tensors='pt',
        padding=True,
        truncation=True,
        max_length=MAX_LEN
    )

    input_ids = encoding['input_ids'].to(device)
    attention_mask = encoding['attention_mask'].to(device)

    model.eval()
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        predictions = torch.argmax(outputs.logits, dim=2)

    word_predictions = []
    predicted_labels = predictions[0].cpu().numpy()
    token_to_word = encoding.word_ids()

    current_word_index = -1
    current_word_predictions = []

    for token_index, word_index in enumerate(token_to_word):
        if word_index is None:
            continue

        if word_index != current_word_index:
            if current_word_predictions:
                word_label = max(set(current_word_predictions), key=current_word_predictions.count)
                word_predictions.append((words[current_word_index], id2label[word_label]))
            current_word_predictions = []
            current_word_index = word_index

        current_word_predictions.append(predicted_labels[token_index])

    if current_word_predictions:
        word_label = max(set(current_word_predictions), key=current_word_predictions.count)
        word_predictions.append((words[current_word_index], id2label[word_label]))

    return word_predictions

"""### download Lid-HinEng Code switching Dataset files"""

!wget -O lid_hineng.zip --no-check-certificate -r 'https://drive.google.com/uc?export=download&id=1QO9HeTSnteLadeYf_wg2aaXDZNaLa25P'
# https://drive.google.com/file/d/1QO9HeTSnteLadeYf_wg2aaXDZNaLa25P/view?usp=sharing

!unzip "/content/lid_hineng.zip" -d "/content/"

"""## Load and tokenize data"""

# Load data
print("Loading data...")
train_texts, train_labels = read_conll_file('lid_hineng/train.conll')
dev_texts, dev_labels = read_conll_file('lid_hineng/dev.conll')

# Initialize tokenizer and model
print("Initializing tokenizer and model...")
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-multilingual-cased')
model = DistilBertForTokenClassification.from_pretrained(
    'distilbert-base-multilingual-cased',
    num_labels=len(label2id),
    id2label=id2label,
    label2id=label2id
)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
model.to(device)

from torchinfo import summary
summary(model)

# Create datasets
print("Preparing datasets...")
train_dataset = CONLLDataset(train_texts, train_labels, tokenizer, MAX_LEN)
dev_dataset = CONLLDataset(dev_texts, dev_labels, tokenizer, MAX_LEN)

train_dataloader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)
dev_dataloader = DataLoader(dev_dataset, batch_size=VALID_BATCH_SIZE)

# Training setup
optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)
total_steps = len(train_dataloader) * EPOCHS
scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, total_iters=total_steps)

# Training loop
print("Starting training...")
best_f1 = 0
for epoch in range(EPOCHS):
    print(f"Epoch {epoch + 1}/{EPOCHS}")
    train_loss = train_epoch(model, train_dataloader, optimizer, device, scheduler)
    print(f"Training loss: {train_loss}")

    print("Evaluating...")
    eval_report = evaluate(model, dev_dataloader, device)
    print(eval_report)

# Save the model
print("Saving model...")
model.save_pretrained('./code_switching_model')
tokenizer.save_pretrained('./code_switching_model')

"""## Loading the saved model and Making predictions"""

import torch
from transformers import DistilBertTokenizerFast, DistilBertForTokenClassification
from typing import List, Tuple

def load_model_and_predict(text_file: str, model_path: str = './code_switching_model'):
    """
    Load a saved model and tokenizer, then make predictions on text from a file.

    Args:
        text_file (str): Path to the file containing text to predict (e.g., test.conll)
        model_path (str): Path to the saved model

    Returns:
        List of predictions for each sentence
    """
    # Set device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    try:
        # Load model and tokenizer
        print("Loading model and tokenizer...")
        model = DistilBertForTokenClassification.from_pretrained(model_path)
        tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)

        model.to(device)
        model.eval()

        # Read test file
        print(f"Reading text from {text_file}...")
        sentences = []
        current_sentence = []

        with open(text_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line.startswith('# sent_enum'):
                    if current_sentence:
                        sentences.append(' '.join(current_sentence))
                        current_sentence = []
                elif line and not line.startswith('#'):
                    word = line.split('\t')[0]
                    current_sentence.append(word)

        if current_sentence:
            sentences.append(' '.join(current_sentence))

        # Make predictions
        all_predictions = []
        print("Making predictions...")
        for sentence in sentences:
            predictions = predict_language_tags(model, tokenizer, sentence, device)
            all_predictions.append(predictions)

            # Print predictions for each sentence
            print(f"\nInput: {sentence}")
            print("Predictions:")
            for word, lang in predictions:
                print(f"{word}: {lang}")
            print("-" * 50)

        return all_predictions

    except Exception as e:
        print(f"Error: {e}")
        return None

def predict_language_tags(model, tokenizer, text: str, device) -> List[Tuple[str, str]]:
    """
    Predict language tags for each word in the input text.
    Returns a list of (word, language) tuples.
    """
    # Tokenize the text
    words = text.split()
    encoding = tokenizer(
        words,
        is_split_into_words=True,
        return_tensors='pt',
        padding=True,
        truncation=True,
        max_length=128
    )

    # Move inputs to device
    input_ids = encoding['input_ids'].to(device)
    attention_mask = encoding['attention_mask'].to(device)

    # Get predictions
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        predictions = torch.argmax(outputs.logits, dim=2)

    # Process predictions
    word_predictions = []
    predicted_labels = predictions[0].cpu().numpy()
    token_to_word = encoding.word_ids()

    current_word_index = -1
    current_word_predictions = []

    for token_index, word_index in enumerate(token_to_word):
        if word_index is None:
            continue

        if word_index != current_word_index:
            if current_word_predictions:
                word_label = max(set(current_word_predictions), key=current_word_predictions.count)
                word_predictions.append((words[current_word_index], id2label[word_label]))
            current_word_predictions = []
            current_word_index = word_index

        current_word_predictions.append(predicted_labels[token_index])

    if current_word_predictions:
        word_label = max(set(current_word_predictions), key=current_word_predictions.count)
        word_predictions.append((words[current_word_index], id2label[word_label]))

    return word_predictions

test_file = 'lid_hineng/test.conll'

# Load model and make predictions
predictions = load_model_and_predict(test_file)

if predictions:
    print(f"\nTotal sentences processed: {len(predictions)}")

"""## Visualize the prediction"""

def visualize_predictions(word_predictions: List[Tuple[str, str]]) -> str:
    """
    Create a colored visualization of the predictions.
    Returns a string with color-coded predictions.
    """
    color_map = {
        'EN': '\033[94m',  # Blue for English
        'HI': '\033[92m',  # Green for Hindi
        'O': '\033[0m'     # Default color for Other
    }
    end_color = '\033[0m'

    result = []
    for word, lang in word_predictions:
        result.append(f"{color_map[lang]}{word}{end_color}")

    return ' '.join(result)

test_text = "Mera name Rahul hai and I Love eating daal chawal ."
predictions =  predict_language_tags(model, tokenizer,test_text ,device)

predictions

print(visualize_predictions(predictions))

"""## Evaluation report on unseen dataset"""

# Use the NER-style approach if:

# You want to keep using seqeval for consistency
# You need sequence-level metrics
# You want to maintain compatibility with NER evaluation tools
def evaluate(model, data_loader, device):
    model.eval()
    predictions = []
    true_labels = []

    with torch.no_grad():
        for batch in data_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels']

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            pred = torch.argmax(logits, dim=2)

            pred_list = pred.detach().cpu().numpy()
            labels_list = labels.cpu().numpy()

            for p, l, mask in zip(pred_list, labels_list, attention_mask):
                pred_labels = [convert_to_ner_tag(id2label[p_i]) for idx, (p_i, m) in enumerate(zip(p, mask))
                              if m == 1 and l[idx] != -100]
                true_labels_subset = [convert_to_ner_tag(id2label[l_i]) for l_i in l if l_i != -100]

                predictions.append(pred_labels)
                true_labels.append(true_labels_subset)

    return classification_report(true_labels, predictions)

def convert_to_ner_tag(tag):
    """Convert plain language tags to NER format"""
    if tag in ['EN', 'HI']:
        return f'B-{tag}'  # or could use 'I-{tag}' - the choice depends on your needs
    return 'O'

eval_report = evaluate(model, dev_dataloader, device)
print(eval_report)

# 'I' likely stands for "In-Language" or "Indic" (possibly indicating Hindi or other Indic languages)
# 'N' likely stands for "Non-Indic" or possibly "English"

"""This warning appears because the seqeval library is designed for Named Entity Recognition (NER) tasks, which typically use a specific format for tags: B-XXX (Beginning), I-XXX (Inside), O (Outside). When we use plain tags like EN and HI, seqeval warns us that these don't follow the standard NER format."""

from sklearn.metrics import classification_report
# doesn't rely on NER conventions
# Use the simple approach if:
# You just need token-level metrics
# You want to avoid the warnings
# You don't need sequence-level evaluation
import sklearn
import numpy as np

def evaluateNonNER(model, data_loader, device):
    model.eval()
    all_predictions = []
    all_labels = []

    with torch.no_grad():
        for batch in data_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels']

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            pred = torch.argmax(logits, dim=2)

            pred_list = pred.detach().cpu().numpy()
            labels_list = labels.cpu().numpy()
            attention_mask_list = attention_mask.cpu().numpy()

            for p, l, mask in zip(pred_list, labels_list, attention_mask_list):
                valid_indices = (mask == 1) & (l != -100)
                all_predictions.extend(p[valid_indices])
                all_labels.extend(l[valid_indices])

    # Convert numeric predictions and labels to 'I' and 'N'
    pred_tags = ['I' if p == label2id['HI'] else 'N' for p in all_predictions]
    true_tags = ['I' if l == label2id['HI'] else 'N' for l in all_labels]

    # Generate classification report
    report = classification_report(true_tags, pred_tags, digits=2)
    return report

print("Evaluating...")
eval_report = evaluateNonNER(model, dev_dataloader, device)
print(eval_report)

